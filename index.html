<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sanjay Haresh</title>

  <meta name="author" content="Sanjay Haresh">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VXL1KY0X4E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-VXL1KY0X4E');
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sanjay Haresh</name>
                  </p>
                  <p>I am a Senior Machine Learning Researcher at Qualcomm AI Research working in <a href="http://www.iro.umontreal.ca/~memisevr/">Roland Memisevic's</a> group on the intersection of large vision language models and robotics. Before that, I completed my MSc. (Thesis) at <a href="https://sfu.ca">Simon Fraser University (SFU)</a>, where I was advised
                    by <a href="https://msavva.github.io/">Prof. Manolis Savva </a>.
                  </p>
                  <p>
                    Prior to SFU, I worked at <a href="https://retrocausal.ai/">Retrocausal</a> for 2 years as a Research
                    Engineer (Computer Vision) under the supervision of <a
                      href="https://cs.adelaide.edu.au/~huy/home.php">Dr. Quoc Huy Tran</a> and <a
                      href="http://www.zeeshanzia.com/">Dr. Zeeshan Zia</a>.
                  </p>
                  <p>
                    In 2019, I completed my undergrad Computer Science from <a
                      href="https://www.nu.edu.pk/">FAST-NUCES</a> Karachi, Pakistan, where I worked on Class Imbalance
                    under the guidance of Prof. Tahir Syed.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sanjayharesh@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/SanjayCV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=boFO-7gAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/SanjayHaresh">Twitter</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/sanjay2.jpg"><img style="width:100%;max-width:100%;border-radius:5%" alt="profile photo"
                      src="images/sanjay2.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications and Preprints</heading>
                  <p>
                    Papers are in reverse chronological order. '*' denotes equal contribution.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/livemamba.jpg' width="170">
                    </div>
                    <img src='images/livemamba.jpg' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://apratimbh.github.io/livecook/">
                    <papertitle>Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?</papertitle>
                  </a>
                  <br>
                  <a href="https://apratimbh.github.io/">Apratim Bhattacharyya*</a>,
                  <a href="https://www.bicheng-xu.com/">Bicheng Xu*</a>,
                  <strong>Sanjay Haresh</strong>,
                  <a href="https://www.linkedin.com/in/rpourreza/">Reza Pourreza</a>,
                  <a href="https://litianliu.github.io/">Litian Liu</a>,
                  <a href="https://www.linkedin.com/in/sunny-panchal/">Sunny Panchal</a>,
                  <a href="https://www.cs.ubc.ca/~lsigal/">Leonid Sigal</a>,
                  <a href="https://www-labs.iro.umontreal.ca/~memisevr/">Roland Memisevic</a>
                  <br>
                  <em>NeurIPS</em>, 2025
                  <br>
                  <a href="https://apratimbh.github.io/livecook/">project page</a>
                  /
                  <a href="https://openreview.net/pdf?id=G6K2NepP7S">Openreview</a>
                  <p>We present Qualcomm Interactive Cooking, a dataset and a benchmark, to enable development of live step-by-step task guidance by situated AI assistants.</p>
                </td>
                
              </tr>
              
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/oat_vla_cropped.gif' width="170">
                    </div>
                    <img src='images/oat_vla_cropped.gif' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2509.23655">
                    <papertitle>Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action Models</papertitle>
                  </a>
                  <br>
                  <a href="https://rokas-bendikas.github.io/">Rokas Bendikas*</a>,
                  <a href="https://www.linkedin.com/in/daniel-fontijne">Daniel Dijkman*</a>,
                  <a href="https://mlpeschl.com/">Markus Peschl</a>,
                  <strong>Sanjay Haresh</strong>,
                  <a href="https://mazpie.github.io/">Pietro Mazzaglia</a>,
                  <br>
                  <em>CoRL</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2509.23655">arXiv</a>
                  <p>We introduce Oat-VLA, a VLA with Object-Agent-centric tokenization which drastically reduces the number of vision tokens enabling ~2x faster training than OpenVLA while outperforming it on real-world tasks.</p>
                </td>
                
                </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/clevrskills.png' width="170">
                    </div>
                    <img src='images/clevrskills.png' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2411.09052">
                    <papertitle>ClevrSkills: Compositional Language And Visual Reasoning in Robotics</papertitle>
                  </a>
                  <br>
                  <strong>Sanjay Haresh</strong>,
                  <a href="https://www.linkedin.com/in/daniel-fontijne">Daniel Dijkman</a>,
                  <a href="https://apratimbh.github.io/">Apratim Bhattacharyya</a>,
                  <a href="https://www-labs.iro.umontreal.ca/~memisevr/">Roland Memisevic</a>
                  <br>
                  <em>NeurIPS (Datasets and Benchmarks Track)</em>, 2024
                  <br>
                  <a href="https://github.com/Qualcomm-AI-research/ClevrSkills/tree/v0?tab=readme-ov-file#getting-started">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2411.09052">arXiv</a>
                  <p>We present a benchmark for compositional learning in robotics. The benchmark consists of 33 manipulation tasks over 3 levels of compositionality. We also open-source a large dataset of ground-truth trajectories generated using oracle solvers.</p>
                </td>
                
                </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/egoexo4d.png' width="170">
                    </div>
                    <img src='images/egoexo4d.png' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2311.18259">
                    <papertitle>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</papertitle>
                  </a>
                  <br>
                  <a href="https://www.cs.utexas.edu/~grauman/">Kristen Grauman</a>,
                  ...
                  <strong>Sanjay Haresh</strong>,
                  <a href="https://sammaoys.github.io/">Yongsen Mao</a>,
                  <a href="https://msavva.github.io/">Manolis Savva</a>,
                  ...
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://ego-exo4d-data.org/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2311.18259">arXiv</a>
                  <p>We present Ego-Exo4D, a diverse, large-scale multimodal multiview video dataset and benchmark challenge to push the frontier of first-person video understanding of skilled human activity.</p>
                </td>
                
                </tr>
                
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='malle_image'>
                        <img src='images/hssd.png' width="170">
                      </div>
                      <img src='images/hssd.png' width="170">
                    </div>
                    <script type="text/javascript">
                      function malle_start() {
                        document.getElementById('malle_image').style.opacity = "1";
                      }
  
                      function malle_stop() {
                        document.getElementById('malle_image').style.opacity = "0";
                      }
                      malle_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2306.11290">
                      <papertitle>Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</papertitle>
                    </a>
                    <br>
                    <a href="https://mukulkhanna.github.io/">Mukul Khanna*</a>,
                    <a href="https://sammaoys.github.io/">Yongsen Mao*</a>,
                    <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>,
                    <strong>Sanjay Haresh</strong>,
                    <a href="https://cs.stanford.edu/~bps/">Brennan Shacklett</a>,
                    <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                    <a href="https://www.linkedin.com/in/alexander-clegg-68336839/">Alexander Clegg</a>,
                    <a href="https://www.ericundersander.com/">Eric Undersander</a>,
                    <a href="https://angelxuanchang.github.io/">Angel Chang</a>,
                    <a href="https://msavva.github.io/">Manolis Savva</a>
                    <br>
                    <em>CVPR</em>, 2024
                    <br>
                    <a href="https://3dlg-hcvc.github.io/hssd/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2306.11290">arXiv</a>
                    <p>We present the Habitat Synthetic Scene Dataset, a dataset of 211 high-quality 3D scenes, and use it to investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects.</p>
                  </td>
                
                </tr>
                
                <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/3dhoi.png' width="170">
                    </div>
                    <img src='images/3dhoi.png' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2209.05612.pdf">
                    <papertitle>Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of
                      Approaches and Challenges</papertitle>
                  </a>
                  <br>
                  <strong>Sanjay Haresh</strong>,
                  <a href="https://www.linkedin.com/in/xiaohao-sun-237537195/">Xiaohao Sun</a>,
                  <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>,
                  <a href="https://angelxuanchang.github.io/">Angel Chang</a>,
                  <a href="https://msavva.github.io/">Manolis Savva</a>
                  <br>
                  <em>3DV</em>, 2022
                  <br>
                  <a href="https://3dlg-hcvc.github.io/3dhoi/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2209.05612.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We canonicalize the task of reconstruction 3D human object from videos and benchmark 5 families of methods on the task.
                  </p>
                </td>


              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/iros22.png' width="170">
                    </div>
                    <img src='images/iros22.png' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2206.15031.pdf">
                    <papertitle>Timestamp-Supervised Action Segmentation with Graph Convolutional Networks</papertitle>
                  </a>
                  <br>
                  <a href="">Hamza Khan</a>
                  <strong>Sanjay Haresh</strong>,
                  <a href="">Awais Ahmed</a>,
                  <a href="">Shakeeb Siddiqui</a>,
                  <a href="https://www.linkedin.com/in/andreykonin/"> Andrey Konin </a>,
                  <a href="http://www.zeeshanzia.com/">M. Zeeshan Zia</a>,
                  <a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran</a>
                  <br>
                  <em>IROS</em>, 2022
                  <br>
                  <a href="https://retrocausal.ai/timestamps">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2206.15031.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We leverage graph convolutional networks to propagate timestamp labels to the whole video resulting in a 97% reduction of required labels.
                  </p>
                </td>


              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/cvpr22.gif' width="170">
                    </div>
                    <img src='images/cvpr22.gif' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2105.13353.pdf">
                    <papertitle>Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering
                    </papertitle>
                  </a>
                  <br>
                  <strong>Sanjay Haresh*</strong>,
                  <a href="https://sateeshkumar21.github.io/">Sateesh Kumar*</a>,
                  <a href="">Awais Ahmed</a>,
                  <a href="https://www.linkedin.com/in/andreykonin/"> Andrey Konin </a>,
                  <a href="http://www.zeeshanzia.com/">M. Zeeshan Zia</a>,
                  <a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran</a>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://retrocausal.ai/unsupervised">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2105.13353.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We proposed temporal optimal transport for jointly learning representations and performing online clustering in an unsupervised manner.
                  </p>
                </td>


              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/cvpr21.gif' width="170">
                    </div>
                    <img src='images/cvpr21.gif' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2103.17260">
                    <papertitle>Learning by Aligning Video in Time</papertitle>
                  </a>
                  <br>
                  <strong>Sanjay Haresh*</strong>,
                  <a href="https://sateeshkumar21.github.io/">Sateesh Kumar*</a>,
                  <a href="https://campar.in.tum.de/Main/HuseyinCoskun">Huseyin Coskun</a>,
                  <a href="https://scholar.google.com/citations?user=3hxAYE8AAAAJ&hl=en"> Shahram N. Syed</a>,
                  <a href="https://www.linkedin.com/in/andreykonin/"> Andrey Konin </a>,
                  <a href="http://www.zeeshanzia.com/">M. Zeeshan Zia</a>,
                  <a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran</a>
                  <br>
                  <em>CVPR</em>, 2021
                  <br>
                  <a href="https://retrocausal.ai/alignment">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2103.17260">arXiv</a>
                  <p></p>
                  <p>
                    Good frame representations can be learned by learning global alignment across pairs of videos via differentiable dynamic time warping.
                  </p>
                </td>


              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/iv20.gif' width="170">
                    </div>
                    <img src='images/iv20.gif' width="170">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2004.05261.pdf">
                    <papertitle>Towards Anomaly Detection in Dashcam Videos</papertitle>
                  </a>
                  <br>
                  <strong>Sanjay Haresh*</strong>,
                  <a href="https://sateeshkumar21.github.io/">Sateesh Kumar*</a>,
                  <a href="http://www.zeeshanzia.com/">M. Zeeshan Zia</a>
                  <a href="https://cs.adelaide.edu.au/~huy/home.php">Quoc-Huy Tran</a>
                  <br>
                  <em>IV</em>, 2020
                  <br>
                  <a href="https://www.youtube.com/watch?v=bnzCsO6WL0w">talk</a>
                  /
                  <a href="https://arxiv.org/pdf/2004.05261.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We curated a large dataset of dashcam videos for road anomalies understanding. We proposed an object-object interaction
                    reasoning approach for detecting anomalies without additional supervision.
                  </p>
                </td>


              </tr>


            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td>
                  <br>
                  <p style="text-align:right">
                    <font size="2"> Website layout is from <a href="https://jonbarron.info/">
                        <font size="2">Jon Barron</font>
                      </a></font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

</body>

</html>